{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:00.936144400Z",
     "start_time": "2023-09-23T14:23:55.649547600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import json\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import os\n",
    "import spacy\n",
    "width = 256\n",
    "height = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:16.522307100Z",
     "start_time": "2023-09-23T14:24:15.865063600Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=1):\n",
    "        self.itos = {0:'<sos>', 1:'<eos>', 2:'<pad>', 3:'<unk>'}\n",
    "        self.stoi = {j:i for i,j in self.itos.items()}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.loss_weight = {0:0,1:0,2:0,3:0}\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(sentence):\n",
    "        return [tok.text.lower() for tok in spacy_en.tokenizer(sentence)]\n",
    "    \n",
    "    def build_vocab(self, sentences:list):\n",
    "        frequencies = {}\n",
    "        for sentence in sentences:\n",
    "            for word in Vocabulary.tokenizer(sentence):\n",
    "                if word not in frequencies.keys():\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "        \n",
    "        idx = 4\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.loss_weight[idx] = 100/freq\n",
    "                self.itos[idx] = word\n",
    "                self.stoi[word] = idx\n",
    "                idx += 1\n",
    "        self.loss_weight[self.stoi['<eos>']] = 100 / len(self.stoi)        \n",
    "    def numericalize(self,sentence):\n",
    "        return [self.stoi[word] if word in self.stoi.keys() else self.stoi['<unk>']\n",
    "                for word in Vocabulary.tokenizer(sentence)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:18.022931300Z",
     "start_time": "2023-09-23T14:24:17.988023200Z"
    }
   },
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "class ImageCaptionDateset(Dataset):\n",
    "    def __init__(self,split='train'):\n",
    "        super(ImageCaptionDateset,self).__init__()\n",
    "\n",
    "        if split == 'train':\n",
    "            self.directory = './train/'\n",
    "            self.images = os.listdir(self.directory)\n",
    "            with open('./annotations/train.json','rb') as f:\n",
    "                data = json.load(f)\n",
    "        else :\n",
    "            self.directory = './val/'\n",
    "            self.images = os.listdir(self.directory)\n",
    "            with open('./annotations/val.json','rb') as f:\n",
    "                data = json.load(f)\n",
    "        self.captions = data['annotations']\n",
    "        \n",
    "        self.transforms = transforms.Compose([transforms.Resize((height,width)),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(mean,std)])\n",
    "        \n",
    "        self.vocab = Vocabulary()\n",
    "        self.vocab.build_vocab([annotation['caption'] for annotation in self.captions])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(os.path.join(self.directory,self.images[index]))\n",
    "        image = self.transforms(image)\n",
    "        caption = '<sos>'\n",
    "        for i in range(5*index,5*index+5):\n",
    "            caption += self.captions[i]['caption']\n",
    "\n",
    "        caption += '<eos>'\n",
    "        caption = torch.tensor(self.vocab.numericalize(caption))\n",
    "\n",
    "        return image, caption\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.captions[-1]['image_id']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:35.942814300Z",
     "start_time": "2023-09-23T14:24:35.909903300Z"
    }
   },
   "outputs": [],
   "source": [
    "class collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def __call__(self, batch:list):\n",
    "        images = [data[0] for data in batch]\n",
    "        captions = [data[1] for data in batch]\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        captions = pad_sequence(captions,True,self.pad_idx)\n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:41.710755800Z",
     "start_time": "2023-09-23T14:24:37.213577500Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_shape = (batch_size,3,height,width)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "trainset = ImageCaptionDateset('train')\n",
    "#testset = ImageCaptionDateset('test')\n",
    "\n",
    "trainLoader = DataLoader(trainset,batch_size,shuffle=True,collate_fn=collate(pad_idx=trainset.vocab.stoi['<pad>']))\n",
    "#testLoader = DataLoader(testset,batch_size,shuffle=True,collate_fn=collate(pad_idx=trainset.vocab.stoi['<pad>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:47.028352800Z",
     "start_time": "2023-09-23T14:24:46.977491100Z"
    }
   },
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel,  m=2):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.lin = nn.Linear(in_channel* m**2,out_channel)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        # x has shape B x C x h x w\n",
    "        x = x.unfold(-2,self.m,self.m).unfold(-2,self.m,self.m) # B x C x h/m x w/m x m x m\n",
    "        x = x.permute(0,2,3,1,4,5).flatten(3,-1) # B x h/m x w/m x c * m^2\n",
    "        x = self.lin(x)\n",
    "        return x.permute(0,3,1,2) # B x out_channel x h/m x w/m\n",
    "\n",
    "class W_MHA(nn.Module):\n",
    "    def __init__(self,shifted:bool, in_channel,num_head, window_size):\n",
    "        super().__init__()\n",
    "        self.shifted = shifted\n",
    "        self.window_size = window_size\n",
    "        self.mha = nn.MultiheadAttention(in_channel,num_head,batch_first=True)\n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        # x has shape B x C x h x w\n",
    "        if not(self.shifted):\n",
    "            B, C, h, w = x.shape\n",
    "            x = x.unfold(-2,self.window_size,self.window_size).unfold(-2,self.window_size,self.window_size).reshape((B,C,-1,self.window_size * self.window_size))\n",
    "            x = x.permute(0,2,3,1).reshape(-1,self.window_size*self.window_size,C)\n",
    "            x,_ = self.mha(x,x,x.clone())\n",
    "            return x.reshape(B,h//self.window_size,w//self.window_size,self.window_size,self.window_size,C).permute(0,5,1,3,2,4).reshape(B,C,h,w)\n",
    "        else:\n",
    "            padding = self.window_size // 2\n",
    "            x = F.pad(x,(padding,padding,padding,padding))\n",
    "            B, C, h, w = x.shape\n",
    "            x = x.unfold(-2,self.window_size,self.window_size).unfold(-2,self.window_size,self.window_size).reshape((B,C,-1,self.window_size * self.window_size))\n",
    "            x = x.permute(0,2,3,1).reshape(-1,self.window_size*self.window_size,C)\n",
    "            x,_ = self.mha(x,x,x.clone(),key_padding_mask= x[:,:,0]==0)\n",
    "            return x.reshape(B,h//self.window_size,w//self.window_size,self.window_size,self.window_size,C).permute(0,5,1,3,2,4).reshape(B,C,h,w)[:,:,padding:-padding,padding:-padding]\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SwinTransformer,self).__init__()\n",
    "        # block 1\n",
    "        _, C, h, w = input_shape\n",
    "        self.LN1_1 = nn.LayerNorm(C)\n",
    "        self.wmsa1 = W_MHA(False,C,8,8)\n",
    "        self.LN1_2 = nn.LayerNorm(C)\n",
    "        self.mlp1 = nn.Sequential(nn.Linear(C,4*C),nn.GELU(),nn.Linear(4*C,C))\n",
    "        \n",
    "        self.LN2_1 = nn.LayerNorm(C)\n",
    "        self.wmsa2 = W_MHA(True,C,8,8)\n",
    "        self.LN2_2 = nn.LayerNorm(C)\n",
    "        self.mlp2 = nn.Sequential(nn.Linear(C,4*C),nn.GELU(),nn.Linear(4*C,C))\n",
    "        \n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        B, C, h, w = x.shape\n",
    "        x = x.permute(0,2,3,1) # B x h x w x C\n",
    "        y = x\n",
    "        x = self.LN1_1(x)\n",
    "        x = self.wmsa1(x.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        x += y\n",
    "        y = x\n",
    "        x = self.LN1_2(x)\n",
    "        x = self.mlp1(x)\n",
    "        x += y\n",
    "        \n",
    "        y = x\n",
    "        x = self.LN2_1(x)\n",
    "        x = self.wmsa2(x.permute(0,3,1,2)).permute(0,2,3,1)\n",
    "        x += y\n",
    "        y = x\n",
    "        x = self.LN2_2(x)\n",
    "        x = self.mlp2(x)\n",
    "        x += y\n",
    "        return x.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:51.596009600Z",
     "start_time": "2023-09-23T14:24:51.564095200Z"
    }
   },
   "outputs": [],
   "source": [
    "class featureExtractor(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        B, c, h, w = input_shape\n",
    "        self.C = 96\n",
    "        self.stage1 = nn.Sequential(PatchMerging(c,self.C,4),SwinTransformer((B,self.C,h//4,w//4)))\n",
    "        \n",
    "        self.stage2 = nn.Sequential(PatchMerging(self.C,2*self.C,2),SwinTransformer((B,2*self.C,h//8,w//8)))\n",
    "\n",
    "        self.stage3 = nn.Sequential(PatchMerging(2*self.C,4*self.C,2),SwinTransformer((B,4*self.C,h//16,w//16))\n",
    "                                    ,SwinTransformer((B,4*self.C,h//16,w//16)),SwinTransformer((B,4*self.C,h//16,w//16)))\n",
    "        \n",
    "        self.stage4 = nn.Sequential(PatchMerging(4*self.C,8*self.C,2),SwinTransformer((B,8*self.C,h//32,w//32)))\n",
    "    \n",
    "    def forward(self,x:torch.Tensor):\n",
    "        x = self.stage1(x)\n",
    "        x1 = x\n",
    "        x = self.stage2(x)\n",
    "        x2 = x\n",
    "        x = self.stage3(x)\n",
    "        x3 = x\n",
    "        x4 = self.stage4(x)\n",
    "        return [x1,x2,x3,x4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deformable DETR Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:24:53.615198Z",
     "start_time": "2023-09-23T14:24:53.595249800Z"
    }
   },
   "outputs": [],
   "source": [
    "class Multi_Scale_Cross_Attention(nn.Module):\n",
    "    def __init__(self, device, hidden_channel=256, num_head=8, layer_num=4, keys_num=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.layer_num = 4\n",
    "        self.num_head = 8\n",
    "        self.hidden_channel = hidden_channel\n",
    "        self.keys_num = keys_num\n",
    "\n",
    "        self.sample_points = nn.Sequential(nn.Linear(hidden_channel, 4*hidden_channel), nn.GELU(), nn.Linear(4*hidden_channel, layer_num*keys_num*2),nn.Sigmoid())\n",
    "        self.mlh = nn.MultiheadAttention(hidden_channel,num_head,batch_first=True)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, v:list):\n",
    "        # x has shape B x N x d\n",
    "        # v has shape L x [B x Hl x Wl x d]\n",
    "        feature_maps_shape = torch.tensor([[map.shape[1]-1,map.shape[2]-1] for map in v],device=self.device)\n",
    "        B, N, d = x.shape\n",
    "        sample_points = self.sample_points(x).reshape(B,N,self.layer_num,self.keys_num,2)\n",
    "        sample_points *= feature_maps_shape[None,None,:,None,:]\n",
    "        keys = self.interpolation(v,sample_points) # B x N x LK x d\n",
    "        assert keys.shape == (B,N,self.layer_num*self.keys_num,d)\n",
    "        keys = keys.flatten(1,-2)\n",
    "        mask = torch.ones((N,N*self.layer_num*self.keys_num),dtype=torch.bool)\n",
    "        index = torch.arange(0,N*self.layer_num*self.keys_num).reshape(N,self.layer_num*self.keys_num)\n",
    "        mask[torch.arange(N)[:,None],index] = False\n",
    "        x,_ = self.mlh(x,keys,keys.clone(),attn_mask=mask.to(device))\n",
    "        return x\n",
    "    \n",
    "    def interpolation(self,v:list, sample_points:torch.Tensor):\n",
    "        # sample point has shape B x N x L x K x 2\n",
    "        # v has shape L x [B x Hl x Wl x d]\n",
    "        B, N, _, _,_= sample_points.shape\n",
    "        up_points = torch.ceil(sample_points).type(torch.int)\n",
    "        down_points = torch.floor(sample_points).type(torch.int)\n",
    "        keys = torch.zeros((B,N,self.layer_num,self.keys_num,self.hidden_channel)).to(self.device) # B x N x L x K\n",
    "        batches = torch.arange(0,B)[:,None]\n",
    "\n",
    "        for l in range(self.layer_num):\n",
    "            row_diff_up = 1 - torch.abs(up_points[:,:,l,:,0]-sample_points[:,:,l,:,0])[:,:,:,None] # B x N x K x 1\n",
    "            row_diff_down = 1 - torch.abs(down_points[:,:,l,:,0]-sample_points[:,:,l,:,0])[:,:,:,None]\n",
    "            \n",
    "            col_diff_up = 1 - torch.abs(up_points[:,:,l,:,1]-sample_points[:,:,l,:,1])[:,:,:,None]\n",
    "            col_diff_down = 1 - torch.abs(down_points[:,:,l,:,1]-sample_points[:,:,l,:,1])[:,:,:,None]\n",
    "                       \n",
    "            value1 = v[l][batches,down_points[:,:,l,:,0].reshape(B,-1),down_points[:,:,l,:,1].reshape(B,-1)].reshape(B,N,self.keys_num,self.hidden_channel) # B x N x K x d\n",
    "            value2 = v[l][batches,down_points[:,:,l,:,0].reshape(B,-1),up_points[:,:,l,:,1].reshape(B,-1)].reshape(B,N,self.keys_num,self.hidden_channel)\n",
    "            value3 = v[l][batches,up_points[:,:,l,:,0].reshape(B,-1),down_points[:,:,l,:,1].reshape(B,-1)].reshape(B,N,self.keys_num,self.hidden_channel)\n",
    "            value4 = v[l][batches,up_points[:,:,l,:,0].reshape(B,-1),up_points[:,:,l,:,1].reshape(B,-1)].reshape(B,N,self.keys_num,self.hidden_channel)\n",
    "            \n",
    "            keys[:,:,l,:,:] = row_diff_down * col_diff_down * value1 + row_diff_down * col_diff_up * value2 + row_diff_up * col_diff_down * value3 + row_diff_up * col_diff_up * value4\n",
    "            \n",
    "        return keys.flatten(2,-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:25:00.617034900Z",
     "start_time": "2023-09-23T14:25:00.581130900Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, device, N=100, d=256, num_head=8, layer_num=4, key_num=4):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.d = d\n",
    "        self.device = device\n",
    "        self.self_mha = nn.MultiheadAttention(d,num_head,batch_first=True)\n",
    "        self.cross_mha = Multi_Scale_Cross_Attention(device,d,num_head,layer_num,key_num)\n",
    "        self.LN1 = nn.LayerNorm(d)\n",
    "        self.LN2 = nn.LayerNorm(d)\n",
    "        self.LN3 = nn.LayerNorm(d)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d,4*d),nn.GELU(),nn.Linear(4*d,d))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor, v:list):\n",
    "        # x has shape B x N x d\n",
    "        y = x\n",
    "        x,_ = self.self_mha(x,x,x.clone())\n",
    "        x += y\n",
    "        x = self.LN1(x)\n",
    "        y = x\n",
    "        x = self.cross_mha(x,v)\n",
    "        x += y\n",
    "        x = self.LN2(x)\n",
    "        y = x\n",
    "        x = self.mlp(x)\n",
    "        x += y\n",
    "        x = self.LN3(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, device, N=100, hidden_channel=256, num_head=8, layer_num=4, key_num=4, batch_size=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.queries = nn.Parameter(torch.rand(batch_size,N,hidden_channel))\n",
    "        self.dec1 = DecoderBlock(device,N,hidden_channel,num_head,layer_num,key_num) \n",
    "        self.dec2 = DecoderBlock(device,N,hidden_channel,num_head,layer_num,key_num) \n",
    "        self.dec3 = DecoderBlock(device,N,hidden_channel,num_head,layer_num,key_num) \n",
    "        self.dec4 = DecoderBlock(device,N,hidden_channel,num_head,layer_num,key_num) \n",
    "        self.LN = nn.LayerNorm(hidden_channel)\n",
    "         \n",
    "    def forward(self,v:list):\n",
    "        # v has shape L x [B x dHl x Wl x d]\n",
    "        output = torch.zeros_like(self.queries)\n",
    "        output += self.queries\n",
    "        output= self.LN(output)\n",
    "        output = self.dec1(output,v)\n",
    "        output = self.dec2(output,v)\n",
    "        output = self.dec3(output,v)\n",
    "        output = self.dec4(output,v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Feature Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:25:03.713254400Z",
     "start_time": "2023-09-23T14:25:03.676354800Z"
    }
   },
   "outputs": [],
   "source": [
    "class GridBlock(nn.Module):\n",
    "    def __init__(self, hidden_channel=256, num_head=8):\n",
    "        super().__init__()\n",
    "        self.mlh = nn.MultiheadAttention(hidden_channel,num_head,batch_first=True)\n",
    "        self.LN1 = nn.LayerNorm(hidden_channel)\n",
    "        self.LN2 = nn.LayerNorm(hidden_channel)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_channel,4*hidden_channel),nn.GELU(),nn.Linear(4*hidden_channel,hidden_channel))\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        # x has shape B x H/32 * W/32 x d\n",
    "        y = x\n",
    "        x,_ = self.mlh(x,x,x.clone())\n",
    "        x += y\n",
    "        x = self.LN1(x)\n",
    "        y = x\n",
    "        x = self.mlp(x)\n",
    "        x += y\n",
    "        x = self.LN2(x)\n",
    "        return x\n",
    "\n",
    "class GridNet(nn.Module):\n",
    "    def __init__(self, hidden_channel=256, num_head=8, block_num=3):\n",
    "        super().__init__()\n",
    "        self.LN = nn.LayerNorm(hidden_channel)\n",
    "        self.blocks = nn.Sequential(*[GridBlock(hidden_channel,num_head) for _ in range(block_num)])\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # x has shape B x H/32 * W/32 x d\n",
    "        x = self.LN(x)\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:25:06.923722800Z",
     "start_time": "2023-09-23T14:25:06.899787100Z"
    }
   },
   "outputs": [],
   "source": [
    "class PosionalEncoding(nn.Module):\n",
    "    def __init__(self,d, max_length=1000):\n",
    "        super().__init__()\n",
    "        assert d%2 == 0\n",
    "        self.pe = torch.zeros((max_length,d))\n",
    "        freq = 10000 ** (-2*torch.arange(0,d/2)/d)[None,:]\n",
    "        indexes =  torch.arange(max_length)[:,None]\n",
    "        self.pe[:,0::2] = torch.sin(freq*indexes)\n",
    "        self.pe[:,1::2] = torch.cos(freq*indexes)\n",
    "\n",
    "    def forward(self,x:torch.Tensor):\n",
    "        # x has shape B x N x d\n",
    "        return x + self.pe[None,:x.shape[1],:]\n",
    "\n",
    "class CpationCrossAttention(nn.Module):\n",
    "    def __init__(self, d=256, num_head=8):\n",
    "        super().__init__()\n",
    "        self.mlh_g = nn.MultiheadAttention(d,num_head,batch_first=True)\n",
    "        self.mlh_def = nn.MultiheadAttention(d,num_head,batch_first=True)\n",
    "        self.ling = nn.Sequential(nn.Linear(2*d,d),nn.Sigmoid())\n",
    "        self.lindef = nn.Sequential(nn.Linear(2*d,d),nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x:torch.Tensor, grid_features:torch.Tensor, def_features:torch.Tensor):\n",
    "        # x : B x L x d\n",
    "        # grid_features : B x M x d\n",
    "        # def_features : B x N x d\n",
    "        x_g,_ = self.mlh_g(x,grid_features,grid_features.clone()) # x : B x L x d\n",
    "        x_def,_ = self.mlh_g(x,def_features,def_features.clone()) # x : B x L x d\n",
    "        c_g = self.ling(torch.cat((x,x_g),dim=-1))\n",
    "        c_def = self.lindef(torch.cat((x,x_def),dim=-1)) # x : B x L x d\n",
    "        return x_g * c_g + x_def * c_def + x\n",
    "\n",
    "class Captionlayer(nn.Module):\n",
    "    def __init__(self,device, hidden_channel=256, num_head=8):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.self_mlh = nn.MultiheadAttention(hidden_channel,num_head,batch_first=True)\n",
    "        self.cross_mlh = CpationCrossAttention(hidden_channel,num_head)\n",
    "        self.LN1 = nn.LayerNorm(hidden_channel)\n",
    "        self.LN2 = nn.LayerNorm(hidden_channel)\n",
    "        self.LN3 = nn.LayerNorm(hidden_channel)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_channel,4*hidden_channel),nn.GELU(),nn.Linear(4*hidden_channel,hidden_channel))\n",
    "\n",
    "    def forward(self,x:torch.Tensor, grid_features:torch.Tensor, def_features:torch.Tensor):\n",
    "        # x : B x L x d\n",
    "        L = x.shape[1]\n",
    "        y = x\n",
    "        mask_atn = (~torch.tril(torch.ones((L,L),dtype=torch.bool))).to(self.device)\n",
    "        x,_ = self.self_mlh(x,x,x.clone(),attn_mask=mask_atn)\n",
    "        x += y\n",
    "        x = self.LN1(x)\n",
    "        y = x\n",
    "        x = self.cross_mlh(x,grid_features,def_features)\n",
    "        x += y\n",
    "        x = self.LN2(x)\n",
    "        y = x\n",
    "        x = self.mlp(x)\n",
    "        x += y\n",
    "        x = self.LN3(x)\n",
    "        return x\n",
    "\n",
    "class CaptionGen(nn.Module):\n",
    "    def __init__(self, device, vocab:Vocabulary, d=256, num_head=8):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.pe = PosionalEncoding(d)\n",
    "        self.embedding = nn.Embedding(len(vocab),d,vocab.stoi['<pad>'],device=device)\n",
    "        self.layer1 = Captionlayer(device,d,num_head)\n",
    "        self.layer2 = Captionlayer(device,d,num_head)\n",
    "        self.layer3 = Captionlayer(device,d,num_head)\n",
    "        self.fc = nn.Linear(d,len(vocab))\n",
    "    \n",
    "    def forward(self,trg:torch.Tensor, grid_features:torch.Tensor, def_features:torch.Tensor):\n",
    "\n",
    "        x = self.embedding(trg) # B x L x d\n",
    "        x = self.pe(x)\n",
    "        x = self.layer1(x,grid_features,def_features)\n",
    "        x = self.layer2(x,grid_features,def_features)\n",
    "        x = self.layer3(x,grid_features,def_features)\n",
    "        x = self.fc(x)\n",
    "        return x # B x L x V\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjDetector(nn.Module):\n",
    "    def __init__(self, input_shape, device, batch_size, hidden_channel=256, class_num=10):\n",
    "        super().__init__(self)\n",
    "        self.backbone = featureExtractor(input_shape)\n",
    "        self.decoder = Decoder(device,batch_size=batch_size)\n",
    "        self.lin1 = nn.Linear(96,256)\n",
    "        self.lin2 = nn.Linear(192,256)\n",
    "        self.lin3 = nn.Linear(384,256)\n",
    "        self.lin4 = nn.Linear(768,256)\n",
    "\n",
    "        self.lin_class = nn.Sequential(nn.Linear(hidden_channel,class_num+1),nn.Softmax(class_num+1))\n",
    "        self.lin_box = nn.Sequential(nn.Linear(hidden_channel,4*hidden_channel),nn.GELU(),nn.Linear(4*hidden_channel,hidden_channel),nn.GELU(),nn.Linear(hidden_channel,4),nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, image:torch.tensor):\n",
    "         # image : B x 3 x H x W\n",
    "        # caption : B x L\n",
    "        v1,v2,v3,v4 = self.backbone(image)\n",
    "        \n",
    "        v1 = self.lin1(v1.permute(0,2,3,1))\n",
    "        v2 = self.lin1(v2.permute(0,2,3,1))\n",
    "        v3 = self.lin1(v3.permute(0,2,3,1))\n",
    "        v4 = self.lin1(v4.permute(0,2,3,1))\n",
    "\n",
    "        image = self.decoder([v1,v2,v3,v4])\n",
    "        classes = self.lin_class(image)\n",
    "        boxes = self.lin_box(image)\n",
    "        return classes, boxes\n",
    "\n",
    "class myLoss(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.background = num_class+1\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        classes, boxes = outputs\n",
    "        classes_tgt, boxes_tgt = targets\n",
    "        B, N, _ = classes.shape\n",
    "        for b in range(B):\n",
    "            mathcing_cost = torch.zeros((N,N))\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected a matrix (2-D array), got a 3 array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Image Captioning\\imageCaptioning.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Image%20Captioning/imageCaptioning.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cost_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m100\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Image%20Captioning/imageCaptioning.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m linear_sum_assignment(cost_matrix)\n",
      "\u001b[1;31mValueError\u001b[0m: expected a matrix (2-D array), got a 3 array"
     ]
    }
   ],
   "source": [
    "cost_matrix = torch.rand(3,100,100)\n",
    "linear_sum_assignment(cost_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:25:10.627132800Z",
     "start_time": "2023-09-23T14:25:10.602199300Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageCaptioning(nn.Module):\n",
    "    def __init__(self, input_shape:tuple, device, batch_size):\n",
    "        super().__init__()\n",
    "        self.tr_backbone = featureExtractor(input_shape)\n",
    "        self.grid = GridNet()\n",
    "        self.objDetector = Decoder(device,batch_size=batch_size)\n",
    "        self.captionGenerator = CaptionGen(device,trainset.vocab)\n",
    "        self.lin1 = nn.Linear(96,256)\n",
    "        self.lin2 = nn.Linear(192,256)\n",
    "        self.lin3 = nn.Linear(384,256)\n",
    "        self.lin4 = nn.Linear(768,256)\n",
    "        \n",
    "    \n",
    "    def forward(self,image:torch.Tensor, caption:torch.Tensor):\n",
    "        # image : B x 3 x H x W\n",
    "        # caption : B x L\n",
    "        v1,v2,v3,v4 = self.tr_backbone(image)\n",
    "        \n",
    "        v1 = self.lin1(v1.permute(0,2,3,1))\n",
    "        v2 = self.lin1(v2.permute(0,2,3,1))\n",
    "        v3 = self.lin1(v3.permute(0,2,3,1))\n",
    "        v4 = self.lin1(v4.permute(0,2,3,1))\n",
    "\n",
    "        grid_features = self.grid(v4)\n",
    "        def_features = self.objDetector([v1,v2,v3,v4])\n",
    "        return self.captionGenerator(caption,grid_features,def_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-23T14:25:14.531527100Z",
     "start_time": "2023-09-23T14:25:14.500610300Z"
    }
   },
   "outputs": [],
   "source": [
    "class Inference(nn.Module):\n",
    "    def __init__(self, vocab:Vocabulary, model_path, device, input_shape, batch_size, beam_size=2, max_length=100, alpha=0.75):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.model = ImageCaptioning(input_shape,device,batch_size).to(device)\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.eval()\n",
    "        self.vocab = vocab\n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, image:torch.Tensor):\n",
    "        # image : B x 3 x H x W\n",
    "        B = image.shape[0]\n",
    "        v1,v2,v3,v4 = self.model.tr_backbone(image)\n",
    "        \n",
    "        v1 = self.model.lin1(v1.permute(0,2,3,1))\n",
    "        v2 = self.model.lin1(v2.permute(0,2,3,1))\n",
    "        v3 = self.model.lin1(v3.permute(0,2,3,1))\n",
    "        v4 = self.model.lin1(v4.permute(0,2,3,1))\n",
    "\n",
    "        grid_features = self.model.grid(v4)\n",
    "        def_features = self.model.objDetector([v1,v2,v3,v4])\n",
    "        completed_captions = self.beam_search(grid_features,def_features)\n",
    "        return completed_captions\n",
    "\n",
    "    \n",
    "    def beam_search(self, grid_features, def_features):\n",
    "        captions = torch.zeros((self.batch_size,self.beam_size,self.max_length,2))\n",
    "        batches = torch.arange(self.batch_size)[:,None]\n",
    "        captions[:,:,0,0] = self.vocab.stoi['<sos>']\n",
    "        captions[:,:,0,1] = 1\n",
    "        vocab_len = len(self.vocab)\n",
    "        beams = [i for i in range(self.beam_size)]\n",
    "        eos = self.vocab.stoi['<eos>']\n",
    "\n",
    "        completed_captions = self.batch_size*[[]]\n",
    "        completed_scores= self.batch_size*[[]]\n",
    "\n",
    "        for t in range(1,self.max_length):\n",
    "            #for i in range(self.beam_size):\n",
    "            #output = self.model.captionGenerator(captions.flatten(0,1)[beams][:,:t],grid_features,def_features,beams)\n",
    "            outputs = []\n",
    "            out_indexes = []\n",
    "            k_sampling = 2 * self.beam_size\n",
    "            for i in beams:\n",
    "                output, out_index = torch.topk(self.model.captionGenerator(captions[:,i,:t,0],grid_features,def_features)[:,-1,:],k_sampling) # B x 2*beam_size\n",
    "                output = softmax(output,dim=-1) * captions[:,i,t-1,[1]] # B x 2*beam_size, unsorted\n",
    "                outputs.append(output) \n",
    "                out_indexes.append(out_index)\n",
    "            \n",
    "            scores , indexes = torch.topk(torch.cat(outputs,dim=-1),self.beam_size) # B x beam_size\n",
    "            out_indexes = torch.cat(out_indexes,dim=-1) # B x 2*beam_size*beam_size\n",
    "            captions= captions[batches,indexes // k_sampling] # update history\n",
    "            new_token = out_indexes[batches,indexes] # B x beam_size, Vocabulary indexes\n",
    "            captions[:,:,t,0] = new_token\n",
    "            captions[:,:,t,1] = scores   \n",
    "\n",
    "            rows, cols = torch.where(new_token==eos)\n",
    "            for j in len(rows):\n",
    "                batch = rows[j]\n",
    "                beam = cols[j]\n",
    "                completed_captions[batch].append(captions[batch,beam,:t+1,0])\n",
    "                completed_scores[batch].append(captions[batch,beam,t,1])\n",
    "            \n",
    "        # Normalizing scores (different caption length)\n",
    "        for batch in range(self.batch_size):\n",
    "            for counter in len(completed_captions[batch]):\n",
    "                den = len(completed_captions[batch][counter]) ** self.alpha\n",
    "                score = completed_scores[batch][counter] \n",
    "                completed_scores[batch][counter] = torch.log(score) / den\n",
    "        \n",
    "        for batch in range(self.batch_size):\n",
    "            scores = torch.cat(completed_scores[batch])\n",
    "            index = torch.argmax(scores)\n",
    "            completed_captions[batch] = completed_captions[batch][torch.argmax(scores).item()]\n",
    "        \n",
    "        return completed_captions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlearning_rate = 0.0001\\nepoch_num = 20\\nload = False\\nmodel = ImageCaptioning(input_shape,device,batch_size).to(device)\\nif load:\\n    model.load_state_dict(torch.load('./ImageCaptioning.pth'))\\n\\nloss_weight = torch.tensor(list(trainset.vocab.loss_weight.values()))\\nloss_criterion = nn.CrossEntropyLoss(loss_weight).to(device)\\n\\noptimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\\n\\nbatch_num = len(trainLoader)\\nlosses = []\\nfor epoch in range(epoch_num):  # loop over the dataset multiple times\\n    model.train()\\n    running_loss = 0.0\\n    tqdm_bar = tqdm(trainLoader, desc=f'Training Epoch {epoch} ', total=int(len(trainLoader)))\\n\\n    for i, (image,caption) in enumerate(tqdm_bar):\\n        image = image.to(device)\\n        caption = caption.to(device)\\n        \\n        output = model(image,caption)\\n\\n        optimizer.zero_grad()\\n        loss = loss_criterion(output[:,0:-1,:].flatten(0,1),caption[:,1:].flatten())\\n        loss.backward()\\n        optimizer.step()\\n        running_loss += loss.item()\\n\\n    running_loss /= batch_num\\n    losses.append(running_loss)\\n    print('epoch : ',epoch,', loss : ',running_loss)\\n    torch.save(model.state_dict(), './ImageCaptioning.pth')\\n    \\nprint('Finished Training')\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "learning_rate = 0.0001\n",
    "epoch_num = 20\n",
    "load = False\n",
    "model = ImageCaptioning(input_shape,device,batch_size).to(device)\n",
    "if load:\n",
    "    model.load_state_dict(torch.load('./ImageCaptioning.pth'))\n",
    "\n",
    "loss_weight = torch.tensor(list(trainset.vocab.loss_weight.values()))\n",
    "loss_criterion = nn.CrossEntropyLoss(loss_weight).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "batch_num = len(trainLoader)\n",
    "losses = []\n",
    "for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    tqdm_bar = tqdm(trainLoader, desc=f'Training Epoch {epoch} ', total=int(len(trainLoader)))\n",
    "\n",
    "    for i, (image,caption) in enumerate(tqdm_bar):\n",
    "        image = image.to(device)\n",
    "        caption = caption.to(device)\n",
    "        \n",
    "        output = model(image,caption)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_criterion(output[:,0:-1,:].flatten(0,1),caption[:,1:].flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    running_loss /= batch_num\n",
    "    losses.append(running_loss)\n",
    "    print('epoch : ',epoch,', loss : ',running_loss)\n",
    "    torch.save(model.state_dict(), './ImageCaptioning.pth')\n",
    "    \n",
    "print('Finished Training')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
